{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSPQP0ZftJYEz6fUTtcQ9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapestand/bpe/blob/master/byte_pair_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "Cx7mixl8Z848"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Data"
      ],
      "metadata": {
        "id": "LcVnkf2hGxvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = \"\"\"\n",
        "    Yeni bir dil Ã¶ÄŸrenmek, zorlu ancak Ã¶dÃ¼llendirici bir deneyim olabilir. Dil Ã¶ÄŸrenme sÃ¼recinde kendi Ã¶ÄŸrenme tarzÄ±nÄ±zÄ± keÅŸfetmek Ã¶nemlidir. Dil Ã¶ÄŸrenmek aynÄ± zamanda farklÄ± kÃ¼ltÃ¼rleri keÅŸfetmenin harika bir yoludur.\n",
        "    Bilgisayar oyunlarÄ±, genÃ§ler arasÄ±nda popÃ¼ler bir eÄŸlence ve zaman geÃ§irme aktivitesidir. Ã–zellikle rekabetÃ§i oyunlar, oyuncular arasÄ±nda bÃ¼yÃ¼k bir ilgi uyandÄ±rmaktadÄ±r.\n",
        "    DoÄŸa yÃ¼rÃ¼yÃ¼ÅŸleri, stres atmanÄ±n ve doÄŸanÄ±n gÃ¼zelliklerini keÅŸfetmenin harika bir yoludur. YÃ¼rÃ¼yÃ¼ÅŸ sÄ±rasÄ±nda doÄŸal bir ortamda olmak, zihinsel saÄŸlÄ±ÄŸa olumlu etkiler yapabilir.\n",
        "    MÃ¼zik, insanlarÄ±n duygusal ifadesi iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r. FarklÄ± mÃ¼zik tÃ¼rleri, farklÄ± duygusal durumlarÄ± yansÄ±tabilir ve dinleyicilere benzersiz bir deneyim sunabilir.\n",
        "    Bilim kurgu romanlarÄ±, hayal gÃ¼cÃ¼nÃ¼ geniÅŸleten ve alternatif gerÃ§ekliklere yol aÃ§an ilginÃ§ hikayeler sunar. Bu tÃ¼r romanlar, okuyucularÄ± farklÄ± dÃ¼nyalara taÅŸÄ±yabilir.\n",
        "    Egzersiz yapmak, fiziksel saÄŸlÄ±ÄŸÄ± artÄ±rmak ve enerji seviyelerini yÃ¼kseltmek iÃ§in etkili bir yÃ¶ntemdir. Egzersiz yapmak aynÄ± zamanda ruh halini iyileÅŸtirebilir ve stresi azaltabilir.\n",
        "    Gastronomi, farklÄ± kÃ¼ltÃ¼rlerin mutfaÄŸÄ±nÄ± keÅŸfetmenin keyifli bir yoludur. Yemek yapmak veya yeni restoranlar denemek, lezzetli bir macera olabilir.\n",
        "    Bilim ve teknoloji, gÃ¼nÃ¼mÃ¼zde hÄ±zla ilerleyen alanlardÄ±r. Yapay zeka ve uzay keÅŸifleri gibi konular, bilim meraklÄ±larÄ± iÃ§in bÃ¼yÃ¼k ilgi Ã§ekicilik taÅŸÄ±r.\n",
        "    Sanat, ifade Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ saÄŸlayan ve estetik deneyimi zenginleÅŸtiren bir yoldur. FarklÄ± sanat tÃ¼rleri, insanlarÄ±n duygusal ve yaratÄ±cÄ± yÃ¶nlerini keÅŸfetmelerine yardÄ±mcÄ± olabilir.\n",
        "    GÃ¶nÃ¼llÃ¼ Ã§alÄ±ÅŸmalar, topluma yardÄ±m etmenin ve sosyal sorumluluk almanÄ±n Ã¶nemli bir yolu olabilir. GÃ¶nÃ¼llÃ¼ olarak zaman ayÄ±rmak, insanlar arasÄ±nda baÄŸlantÄ± kurma fÄ±rsatÄ± sunabilir.\n",
        "\"\"\".split('\\n')\n",
        "\n",
        "# test_corpus = \"\"\"\n",
        "#     BilgisayarğŸ˜Š oyunlarÄ±, genÃ§ler arasÄ±nda popÃ¼ler bir eÄŸlence ve zaman geÃ§irme aktivitesidir. Ã–zellikle rekabetÃ§i oyunlar, oyuncular arasÄ±nda bÃ¼yÃ¼k bir ilgi uyandÄ±rmaktadÄ±r.\n",
        "#     Yeni bir dil Ã¶ÄŸrenmek, zorlu ancak Ã¶dÃ¼llendirici bir deneyim olabilir. Dil Ã¶ÄŸrenme sÃ¼recinde kendi Ã¶ÄŸrenme tarzÄ±nÄ±zÄ± keÅŸfetmek Ã¶nemlidir. Dil Ã¶ÄŸrenmek aynÄ± zamanda farklÄ± kÃ¼ltÃ¼rleri keÅŸfetmenin harika bir yoludur.\n",
        "#     DoÄŸa yÃ¼rÃ¼yÃ¼ÅŸleri, stres atmanÄ±n ve doÄŸanÄ±n gÃ¼zelliklerini keÅŸfetmenin harika bir yoludur. YÃ¼rÃ¼yÃ¼ÅŸ sÄ±rasÄ±nda doÄŸal bir ortamda olmak, zihinsel saÄŸlÄ±ÄŸa olumlu etkiler yapabilir.\n",
        "#     MÃ¼zik, insanlarÄ±n duygusal ifadesi iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r. FarklÄ± mÃ¼zik tÃ¼rleriğŸ˜Š, farklÄ± duygusal durumlarÄ± yansÄ±tabilir ve dinleyicilere benzersiz bir deneyim sunabilir.\n",
        "#     Bilim kurgu romanlarÄ±, hayal gÃ¼cÃ¼nÃ¼ geniÅŸleten ve alternatif gerÃ§ekliklere yol aÃ§an ilginÃ§ hikayeler sunar. Bu tÃ¼r romanlar, okuyucularÄ± farklÄ± dÃ¼nyalara taÅŸÄ±yabilir.\n",
        "#     Egzersiz yapmak, fiziksel saÄŸlÄ±ÄŸÄ± artÄ±rmak ve enerji seviyelerini yÃ¼kseltmek iÃ§in etkili bir yÃ¶ntemdir. Egzersiz yapmak aynÄ± zamanda ruh halini iyileÅŸtirebilir ve stresi azaltabilir.\n",
        "#     Gastronomi, farklÄ± kÃ¼ltÃ¼rlerin mutfaÄŸÄ±nÄ± keÅŸfetmenin keyifli bir yoludur. Yemek yapmak veya yeni restoranlar denemek, lezzetli bir macera olabilir.\n",
        "#     Bilim ve teknoloji, gÃ¼nÃ¼mÃ¼zde hÄ±zla ilerleyen alanlardÄ±r. Yapay zeka ve uzay keÅŸifleri gibi konular, bilim meraklÄ±larÄ± iÃ§in bÃ¼yÃ¼k ilgi Ã§ekicilik taÅŸÄ±r.\n",
        "#     Sanat, ifade Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ saÄŸlayan ve estetik deneyimi zenginleÅŸtiren bir yoldur. FarklÄ± sanat tÃ¼rleri, insanlarÄ±n duygusal ve yaratÄ±cÄ± yÃ¶nlerini keÅŸfetmelerine yardÄ±mcÄ± olabilir.\n",
        "#     GÃ¶nÃ¼llÃ¼ Ã§alÄ±ÅŸmalar, topluma yardÄ±m etmenin ve sosyal sorumluluk almanÄ±n Ã¶nemli bir yolu olabilir. GÃ¶nÃ¼llÃ¼ olarak zaman ayÄ±rmak, insanlar arasÄ±nda baÄŸlantÄ± kurma fÄ±rsatÄ± sunabilir.\n",
        "# \"\"\".split('\\n')"
      ],
      "metadata": {
        "id": "m95AVyLeQGpC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize"
      ],
      "metadata": {
        "id": "0Snor2URHEJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_result(_encoder):\n",
        "    tokenize_res = _encoder.tokenize(test)\n",
        "    y = _encoder.transform([test])\n",
        "    x = _encoder.inverse_transform(y)\n",
        "\n",
        "    print('Text: ')\n",
        "    print(test)\n",
        "    print('Tokenize result: ')\n",
        "    print(tokenize_res)\n",
        "    print('Transform result: ')\n",
        "    print(y)\n",
        "    print('Inverse transform: ')\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "A-gVqkDRHCfw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character Level BPE"
      ],
      "metadata": {
        "id": "GbWZb9WRG3Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import Dict, Iterable, Callable, List, Any, Iterator\n",
        "from itertools import chain\n",
        "from functools import reduce\n",
        "\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "import logging\n",
        "import toolz\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "class BytePairEncoder:\n",
        "    EOW = '__eow'\n",
        "    SOW = '__sow'\n",
        "    UNK = '__unk'\n",
        "    PAD = '__pad'\n",
        "\n",
        "    def __init__(self, vocab_size=1000, log_level=logging.WARNING):\n",
        "        self._logger = logging.getLogger('BytePairEncoderLogger')\n",
        "        self._logger.setLevel(log_level)\n",
        "\n",
        "        self.merges = {}\n",
        "        self.inverse_merges = {}\n",
        "        self.vocab = []\n",
        "        self.inverse_vocab = {}\n",
        "\n",
        "        self.token_mapper = {\n",
        "            BytePairEncoder.SOW: '',\n",
        "            BytePairEncoder.EOW: ' '\n",
        "        }\n",
        "\n",
        "        self.required_tokens = [BytePairEncoder.SOW, BytePairEncoder.EOW, BytePairEncoder.UNK]\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self._logger.debug('Initialized')\n",
        "\n",
        "    def __set_log_level(self, log_level):\n",
        "        self._logger.setLevel(log_level)\n",
        "\n",
        "    def __tokenize_word(self, sentence: str):\n",
        "        return wordpunct_tokenize(sentence)\n",
        "\n",
        "    def __initialize_word_frequencies(self, corpus: List[str]):\n",
        "        vocab = {}\n",
        "        for sentence in corpus:\n",
        "            for word in self.__tokenize_word(sentence):\n",
        "                vocab[word] = vocab.get(word, 0) + 1\n",
        "        self._logger.debug('Word frequency map initialized!')\n",
        "        return vocab\n",
        "\n",
        "    def __initialize_base_vocab(self, word_freqs):\n",
        "        char_freqs = {}\n",
        "        for word, frequency in word_freqs.items():\n",
        "            for char in word:\n",
        "                char_freqs[char] = char_freqs.get(char, 0) + frequency\n",
        "        char_freqs = list(map(lambda x: x[0], sorted(char_freqs.items(), key=lambda x: x[1], reverse=True)))\n",
        "\n",
        "        base_vocab = self.required_tokens + char_freqs\n",
        "        self._logger.debug('Base vocabulary initialized!')\n",
        "        return base_vocab\n",
        "\n",
        "\n",
        "    def __compute_pair_freqs(self, word_freqs, splits):\n",
        "        pair_freqs = {}\n",
        "        for word, freq in word_freqs.items():\n",
        "            split = splits[word]\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(split) - 1):\n",
        "                pair = (split[i], split[i + 1])\n",
        "                pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
        "        return pair_freqs\n",
        "\n",
        "    def __get_most_frequent_pair(self, word_freqs, splits):\n",
        "        pair_freqs =  self.__compute_pair_freqs(word_freqs, splits)\n",
        "        most_frequent_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "        return most_frequent_pair\n",
        "\n",
        "    def __learn_vocab(self, word_freqs, vocab):\n",
        "        merges = {}\n",
        "        splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "        idx = len(vocab)\n",
        "        while len(vocab) < self.vocab_size:\n",
        "            if all(len(tokens) <= 1 for tokens in splits.values()):\n",
        "                self._logger.warning('All words are tokenized. There is no pair to merge. Breaking...')\n",
        "                break\n",
        "\n",
        "            most_frequent_pair = self.__get_most_frequent_pair(word_freqs, splits)\n",
        "            (a, b) = most_frequent_pair\n",
        "            for word in word_freqs:\n",
        "                split = splits[word]\n",
        "                if len(split) == 1:\n",
        "                    continue\n",
        "\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == a and split[i + 1] == b:\n",
        "                        split = split[:i] + [a + b] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[word] = split\n",
        "\n",
        "            merges[most_frequent_pair] = idx\n",
        "            vocab.append(a + b)\n",
        "            idx += 1\n",
        "        self._logger.debug('BPE vocabulary and merge map created!')\n",
        "        return vocab, merges\n",
        "\n",
        "    def fit(self, corpus: List[str]):\n",
        "        word_freqs = self.__initialize_word_frequencies(corpus)\n",
        "        vocab = self.__initialize_base_vocab(word_freqs)\n",
        "\n",
        "        self.vocab, self.merges = self.__learn_vocab(word_freqs, vocab)\n",
        "        self.inverse_vocab = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.inverse_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.__tokenize_word(text)\n",
        "        splits = [[l for l in word] for word in text]\n",
        "\n",
        "        for pair, merge in self.merges.items():\n",
        "            for idx, split in enumerate(splits):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [pair[0] + pair[1]] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[idx] = split\n",
        "\n",
        "        for idx, split in enumerate(splits):\n",
        "            splits[idx] = [BytePairEncoder.SOW] + splits[idx] + [BytePairEncoder.EOW]\n",
        "\n",
        "        return sum(splits, [])\n",
        "\n",
        "    def single_transform(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        encoded = []\n",
        "        for token in tokens:\n",
        "            if token in self.vocab:\n",
        "                encoded.append(self.inverse_vocab[token])\n",
        "            else:\n",
        "                self._logger.debug(f'Character \\'{token}\\' not found in vocabulary, adding UNK token!')\n",
        "                encoded.append(self.inverse_vocab[BytePairEncoder.UNK])\n",
        "        return encoded\n",
        "\n",
        "    def transform(self, list_of_texts):\n",
        "        return [self.single_transform(text) for text in list_of_texts]\n",
        "\n",
        "    def single_inverse_transform(self, tokens):\n",
        "        decoded = ''\n",
        "        for idx in tokens:\n",
        "            token = self.vocab[idx]\n",
        "            decoded += self.token_mapper.get(token, token)\n",
        "        return decoded.strip()\n",
        "\n",
        "    def inverse_transform(self, list_of_tokens):\n",
        "        return [self.single_inverse_transform(tokens) for tokens in list_of_tokens]"
      ],
      "metadata": {
        "id": "lDPkeqrvfyJ_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = BytePairEncoder(vocab_size=500, log_level=logging.DEBUG)\n",
        "encoder.fit(test_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwGf9Q3TQBUo",
        "outputId": "8d5b553c-c6ea-4485-ada9-10f3e743f5b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:BytePairEncoderLogger:Initialized\n",
            "DEBUG:BytePairEncoderLogger:Word frequency map initialized!\n",
            "DEBUG:BytePairEncoderLogger:Base vocabulary initialized!\n",
            "DEBUG:BytePairEncoderLogger:BPE vocabulary and merge map created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor.'\n",
        "# test = 'this text is written in a different language'\n",
        "# test = 'Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor.ğŸ˜Š'\n",
        "# test = 'Thissrasp is ~noxtğŸ˜Š a token.'\n",
        "# test = 'å­¦ç¿’ã‚’å®Œäº†ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã€ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã—ã¾ã™ã€‚'"
      ],
      "metadata": {
        "id": "RpD_iNfeCEXe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_result(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciVKbOUIF2nI",
        "outputId": "d8334320-9fdd-4536-de94-4529ce00bb1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \n",
            "Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor.\n",
            "Tokenize result: \n",
            "['__sow', 'Ã¶ÄŸren', 'm', 'es', 'in', 'i', '__eow', '__sow', 'ta', 'ma', 'm', 'l', 'ay', 'an', '__eow', '__sow', 't', 'o', 'ken', 'iz', 'er', '__eow', '__sow', 'b', 'u', '__eow', '__sow', 'm', 'et', 'n', 'i', '__eow', '__sow', 't', 'o', 'ken', 'lerin', 'e', '__eow', '__sow', 'ay', 'Ä±r', 'Ä±', 'y', 'or', '__eow', '__sow', '.', '__eow']\n",
            "Transform result: \n",
            "[[0, 102, 10, 82, 49, 4, 1, 0, 119, 126, 10, 7, 65, 48, 1, 0, 13, 18, 250, 125, 47, 1, 0, 19, 14, 1, 0, 10, 55, 8, 4, 1, 0, 13, 18, 250, 96, 5, 1, 0, 65, 67, 12, 11, 103, 1, 0, 21, 1]]\n",
            "Inverse transform: \n",
            "['Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte Level BPE"
      ],
      "metadata": {
        "id": "_j6msOExIAwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import Dict, Iterable, Callable, List, Any, Iterator\n",
        "from itertools import chain\n",
        "from functools import reduce\n",
        "\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "import logging\n",
        "import toolz\n",
        "import json\n",
        "import re\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "class ByteLevelBytePairEncoding:\n",
        "\n",
        "    def __init__(self, vocab_size=1000, log_level=logging.WARNING):\n",
        "        self._logger = logging.getLogger('BytePairEncoderLogger')\n",
        "        self._logger.setLevel(log_level)\n",
        "\n",
        "        self.merges = {}\n",
        "        self.inverse_merges = {}\n",
        "        self.vocab = []\n",
        "        self.inverse_vocab = {}\n",
        "\n",
        "        self.EOW = 256\n",
        "        self.SOW = 257\n",
        "        self.UNK = 258\n",
        "        self.PAD = 259\n",
        "\n",
        "        self.token_mapper = {\n",
        "            self.EOW: 32 # Space\n",
        "        }\n",
        "\n",
        "        self.required_tokens = [self.SOW, self.EOW, self.UNK]\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self._logger.debug('Initialized')\n",
        "\n",
        "    def __set_log_level(self, log_level):\n",
        "        self._logger.setLevel(log_level)\n",
        "\n",
        "    def __tokenize_word(self, sentence: str):\n",
        "        return [''.join(word).encode('utf-8') for word in wordpunct_tokenize(sentence)]\n",
        "\n",
        "    def __initialize_word_frequencies(self, corpus: List[str]):\n",
        "        vocab = {}\n",
        "        for sentence in corpus:\n",
        "            for word in self.__tokenize_word(sentence):\n",
        "                vocab[word] = vocab.get(word, 0) + 1\n",
        "        self._logger.debug('Word frequency map initialized!')\n",
        "        return vocab\n",
        "\n",
        "    def __initialize_base_vocab(self, word_freqs):\n",
        "        byte_freqs = {}\n",
        "        for word, frequency in word_freqs.items():\n",
        "            for char in word:\n",
        "                byte_freqs[char] = byte_freqs.get(char, 0) + frequency\n",
        "        byte_freqs = list(map(lambda x: x[0], sorted(byte_freqs.items(), key=lambda x: x[1], reverse=True)))\n",
        "        base_vocab = list(dict.fromkeys(self.required_tokens[:] + byte_freqs + list(range(256))))\n",
        "\n",
        "        self._logger.debug('Base vocabulary initialized!')\n",
        "        return base_vocab\n",
        "\n",
        "\n",
        "    def __compute_pair_freqs(self, word_freqs, splits):\n",
        "        pair_freqs = {}\n",
        "        for word, freq in word_freqs.items():\n",
        "            split = splits[word]\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(split) - 1):\n",
        "                pair = (split[i], split[i + 1])\n",
        "                pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
        "        return pair_freqs\n",
        "\n",
        "    def __get_most_frequent_pair(self, word_freqs, splits):\n",
        "        pair_freqs =  self.__compute_pair_freqs(word_freqs, splits)\n",
        "        most_frequent_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "        return most_frequent_pair\n",
        "\n",
        "    def __learn_vocab(self, word_freqs, vocab):\n",
        "        merges = {}\n",
        "        splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "        new_token_count = 0\n",
        "        start_idx = len(vocab)\n",
        "        while len(vocab) < self.vocab_size:\n",
        "            if all(len(tokens) <= 1 for tokens in splits.values()):\n",
        "                self._logger.warning('All words are tokenized. There is no pair to merge. Breaking...')\n",
        "                break\n",
        "\n",
        "            most_frequent_pair = self.__get_most_frequent_pair(word_freqs, splits)\n",
        "            (a, b) = most_frequent_pair\n",
        "            new_token_count += 1\n",
        "            for word in word_freqs:\n",
        "                split = splits[word]\n",
        "                if len(split) == 1:\n",
        "                    continue\n",
        "\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == a and split[i + 1] == b:\n",
        "                        split = split[:i] + [start_idx + new_token_count] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[word] = split\n",
        "\n",
        "            merges[most_frequent_pair] = start_idx + new_token_count\n",
        "            vocab.append(start_idx + new_token_count)\n",
        "        self._logger.debug('BPE vocabulary and merge map created!')\n",
        "        return vocab, merges\n",
        "\n",
        "    def fit(self, corpus: List[str]):\n",
        "        word_freqs = self.__initialize_word_frequencies(corpus)\n",
        "        vocab = self.__initialize_base_vocab(word_freqs)\n",
        "        self.vocab, self.merges = self.__learn_vocab(word_freqs, vocab)\n",
        "\n",
        "        self.inverse_vocab = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.inverse_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.__tokenize_word(text)\n",
        "        splits = [[l for l in word] for word in text]\n",
        "\n",
        "        for pair, merge in self.merges.items():\n",
        "            for idx, split in enumerate(splits):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[idx] = split\n",
        "\n",
        "        for idx, split in enumerate(splits):\n",
        "            splits[idx] = [self.SOW] + splits[idx] + [self.EOW]\n",
        "        return sum(splits, [])\n",
        "\n",
        "    def transform(self, text_list):\n",
        "        # for text in text_list:\n",
        "        #     yield self.tokenize(text)\n",
        "        return [self.tokenize(text) for text in text_list]\n",
        "\n",
        "    def _inverse_transform_single(self, tokens):\n",
        "        idx = 0\n",
        "        decoded = []\n",
        "        while idx < len(tokens) -1:\n",
        "            token = tokens[idx]\n",
        "            if token in self.inverse_merges:\n",
        "                merges = self.inverse_merges[token]\n",
        "                tokens = tokens[:idx] + [merges[0], merges[1]] + tokens[idx + 1:]\n",
        "            else:\n",
        "                idx += 1\n",
        "                if token in [self.SOW, self.UNK]:\n",
        "                    continue\n",
        "                token = self.token_mapper.get(token, token)\n",
        "                decoded.append(token)\n",
        "        return bytes(decoded).decode('utf-8')\n",
        "\n",
        "    def inverse_transform(self, token_lists):\n",
        "        # for tokens in token_lists:\n",
        "        #     yield self._inverse_transform_single(tokens)\n",
        "        return [self._inverse_transform_single(tokens) for tokens in token_lists]"
      ],
      "metadata": {
        "id": "Pzz_9T4jMK8t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_level_encoder = ByteLevelBytePairEncoding(vocab_size=500, log_level=logging.DEBUG)\n",
        "byte_level_encoder.fit(test_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn3jg3rtoJoq",
        "outputId": "cbd1d571-5271-42e2-af00-f1a6293cf89c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:BytePairEncoderLogger:Initialized\n",
            "DEBUG:BytePairEncoderLogger:Word frequency map initialized!\n",
            "DEBUG:BytePairEncoderLogger:Base vocabulary initialized!\n",
            "DEBUG:BytePairEncoderLogger:BPE vocabulary and merge map created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test = 'Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor.'\n",
        "# test = 'this text is written in a different language'\n",
        "# test = 'Ã¶ÄŸrenmesini tamamlayan tokenizer bu metni tokenlerine ayÄ±rÄ±yor.ğŸ˜Š'\n",
        "# test = 'Thissrasp is ~noxtğŸ˜Š a token.'\n",
        "test = 'å­¦ç¿’ã‚’å®Œäº†ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã€ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã—ã¾ã™ã€‚'"
      ],
      "metadata": {
        "id": "StGdImgMFJYK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_result(byte_level_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8lY4ijy-m-T",
        "outputId": "f3ac75a3-087a-4860-91b9-5801b5b26608"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \n",
            "å­¦ç¿’ã‚’å®Œäº†ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã€ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
            "Tokenize result: \n",
            "[257, 229, 173, 166, 231, 191, 146, 227, 130, 146, 229, 174, 140, 228, 186, 134, 227, 129, 151, 227, 129, 159, 227, 131, 136, 227, 131, 188, 227, 130, 175, 227, 131, 138, 227, 130, 164, 227, 130, 182, 227, 129, 175, 256, 257, 227, 128, 129, 256, 257, 227, 129, 147, 227, 129, 174, 227, 131, 134, 227, 130, 173, 227, 130, 185, 227, 131, 136, 227, 130, 146, 227, 131, 136, 227, 131, 188, 227, 130, 175, 227, 131, 179, 227, 129, 171, 229, 136, 134, 229, 137, 178, 227, 129, 151, 227, 129, 190, 227, 129, 153, 256, 257, 227, 128, 130, 256]\n",
            "Transform result: \n",
            "[[257, 229, 173, 166, 231, 191, 146, 227, 130, 146, 229, 174, 140, 228, 186, 134, 227, 129, 151, 227, 129, 159, 227, 131, 136, 227, 131, 188, 227, 130, 175, 227, 131, 138, 227, 130, 164, 227, 130, 182, 227, 129, 175, 256, 257, 227, 128, 129, 256, 257, 227, 129, 147, 227, 129, 174, 227, 131, 134, 227, 130, 173, 227, 130, 185, 227, 131, 136, 227, 130, 146, 227, 131, 136, 227, 131, 188, 227, 130, 175, 227, 131, 179, 227, 129, 171, 229, 136, 134, 229, 137, 178, 227, 129, 151, 227, 129, 190, 227, 129, 153, 256, 257, 227, 128, 130, 256]]\n",
            "Inverse transform: \n",
            "['å­¦ç¿’ã‚’å®Œäº†ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ ã€ ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã—ã¾ã™ ã€‚']\n"
          ]
        }
      ]
    }
  ]
}